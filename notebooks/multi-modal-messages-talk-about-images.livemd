<!-- livebook:{"file_entries":[{"name":"image1.jpg","type":"attachment"},{"name":"image2.jpg","type":"attachment"}]} -->

# Images: Intro to Multi-Modal Messages

```elixir
Mix.install([
  {:langchain, github: "brainlid/langchain"},
  # {:langchain, "~> 0.2.0"},
  {:kino, "~> 0.12.0"}
])
```

## Image inputs

Before we can interact with an LLM about an image, we need to get the images to work with. There are two image input so we can potentially discuss two images at once.

You can take a photo or upload an image to use.

```elixir
input1 = Kino.Input.image("image 1", format: :jpeg)
```

```elixir
input2 = Kino.Input.image("image 2", format: :jpeg)
```

```elixir
image1 = Kino.Input.read(input1)
image2 = Kino.Input.read(input2)
```

We're going to use the image data as `base64` encoded text. Let's get that ready here:

```elixir
image1_data =
  image1.file_ref
  |> Kino.Input.file_path()
  |> File.read!()
  |> :base64.encode()

image2_data =
  image2.file_ref
  |> Kino.Input.file_path()
  |> File.read!()
  |> :base64.encode()

:ok
```

## LangChain

When the images are ready, we'll setup an LLMChain request to ChatGPT and ask it questions about our images.

**NOTE:** You must provide your own OPENAI_API_KEY in the Livebook secrets to do this.

```elixir
Application.put_env(:langchain, :openai_key, System.fetch_env!("LB_OPENAI_API_KEY"))
```

First, let's setup the model for talking to ChatGPT. For simplicity, we've set it not stream the response back. We'll get the final analysis once it's complete.

**NOTE:** For ChatGPT, image support requires using the `"gpt-4-visual-preview"` model at the time this was created.

```elixir
alias LangChain.ChatModels.ChatOpenAI

openai_chat_model =
  ChatOpenAI.new!(%{model: "gpt-4-1106-vision-preview", temperature: 1, stream: false})
```

Here we setup our messages. The user message contains multiple `ContentPart`s. One is text for our question, then we add two that contain the `base64` encoded image data.

We will submit all of this data together in our request.

**NOTE:** Make sure the `:media` option matches both the image and what is supported by the LLM you are connecting with.

```elixir
alias LangChain.Message
alias LangChain.Message.ContentPart

messages = [
  Message.new_system!("You are a helpful assistant."),
  Message.new_user!([
    ContentPart.text!("Compare and contrast the contents of these images:"),
    ContentPart.image!(image1_data, media: :jpg),
    ContentPart.image!(image2_data, media: :jpg)
  ])
]
```

Everything is ready to make the request!

* We have images
* We setup which LLM we are connecting with
* We combined our images with a user prompt

Now, we'll submit the request to the server and review the response.

```elixir
alias LangChain.Chains.LLMChain

{:ok, _updated_chain, response} =
  %{llm: openai_chat_model, verbose: true}
  |> LLMChain.new!()
  |> LLMChain.add_messages(messages)
  |> LLMChain.run()

IO.puts(response.content)
response.content
```

Awesome! The ability to intermix user text with images opens up new possibilities. And when you look at the amount of code it took to do this, it's really manageable! ðŸ‘

## Anthropic (BONUS)

While we're all setup for it, if you have an Anthropic API key, then we'll submit the same request on Anthropic and see how that behaves.

**NOTE:** You must provide your own ANTHROPIC_API_KEY in the Livebook secrets to do this.

```elixir
Application.put_env(:langchain, :anthropic_key, System.fetch_env!("LB_ANTHROPIC_API_KEY"))
```

Let's setup our Anthropic chat model.

**NOTE:** Keep in mind that different versions of Claude will give different results. You can play with that to find a good cost/accuracy for your specific need.

```elixir
alias LangChain.ChatModels.ChatAnthropic

anthropic_chat_model =
  ChatAnthropic.new!(%{model: "claude-3-opus-20240229", stream: false})
```

Now we run the same messages through an identical LLMChain but passing in the Anthropic chat model.

```elixir
alias LangChain.Chains.LLMChain

{:ok, _updated_chain, response} =
  %{llm: anthropic_chat_model, verbose: true}
  |> LLMChain.new!()
  |> LLMChain.add_messages(messages)
  |> LLMChain.run()

IO.puts(response.content)
response.content
```

Nice! With no code changes, we can make a similar request about images from Anthropic's Claude LLM as well!

<!-- livebook:{"offset":4316,"stamp":{"token":"XCP.FLFLzs3iil0ccF9JMsC_yJSWTr0cSKKPSR442QWS3GXQnrttcUbjO8SNAGvjHokZNB8xcWkNhnP13PCiIIMcm-O6KGnjDcxCG0ytUqUp8G1XC3rXTgdiPGuybEUABhtvjuiLPq-TDJDzk9xEbxZu","version":2}} -->
